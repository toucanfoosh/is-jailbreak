{
  "name": "llm-jailbreak",
  "version": "1.0.4",
  "description": "A simple package to determine if a string is likely to be a jailbreak string for an llm or not.",
  "main": "dist/index.js",
  "files": [
    "dist/**/*"
  ],
  "scripts": {
    "test": "jest",
    "extract": "cd dev && node extract_model.js && cd ..",
    "build": "npm run extract && tsc"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/toucanfoosh/llm-jailbreak"
  },
  "keywords": [
    "llm",
    "jailbreak",
    "prompt",
    "hack",
    "DAN"
  ],
  "author": "Daniel Wu",
  "license": "MIT",
  "dependencies": {
    "@tensorflow/tfjs": "^4.22.0",
    "@tensorflow/tfjs-layers": "^4.22.0",
    "@tensorflow/tfjs-node": "^4.22.0"
  },
  "devDependencies": {
    "fs-extra": "^11.2.0",
    "jest": "^29.7.0",
    "ts-jest": "^29.2.5",
    "typescript": "^5.7.2"
  }
}
